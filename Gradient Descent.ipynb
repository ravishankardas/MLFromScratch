{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da76fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc8c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.a\n",
    "def gradient_descent(gradient,init_,learn_rate,n_iter=50,tol=1e-06):\n",
    "    x = init_\n",
    "    for init_ in range(n_iter):\n",
    "        delta = -learn_rate * gradient(x)\n",
    "        if np.all(np.abs(delta)<=tol):\n",
    "            break\n",
    "        x+=delta\n",
    "    return round(x*1000)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d0f4610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(gradient=lambda v: 2* v + 3 , init_=4.0, learn_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a30cc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.366"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(gradient=lambda v: 2*v**3 - 3*v + 1 , init_=0.1, learn_rate=0.06)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c622f56c",
   "metadata": {},
   "source": [
    "1.b<br>for linear regression we find the loss function using mean squared error(MSE)\n",
    "here x and y and input parameters and and we find the partial derivative with respect to slope(a) and bias(b) for y = ax+b to get the gradient, here $\\overline{y_i}$ is the predicted value and $y_i$ is the actual value\n",
    "\\begin{align*}\n",
    "\\textbf{E}&=\\frac{1}{n}\\sum\\limits_{i=1}^{n}(y_i-\\overline{y_i})^2\\\\\n",
    "\\textbf{E} &= \\frac{1}{n}\\sum\\limits_{i=1}^{n}(y_i-(ax_i + b))^2\\\\\n",
    "\\frac{\\partial E}{\\partial a}&=\\frac{-2}{N}\\sum\\limits_{i=1}^{N}x_i(y_i-(ax_i+b))\\\\\n",
    "\\frac{\\partial E}{\\partial b}&=\\frac{-2}{N}\\sum\\limits_{i=1}^{N}(y_i-(ax_i+b))\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399e6458",
   "metadata": {},
   "source": [
    "below gradient function will take three parameters X,y and pred_y  where X is the  value with which we will calculate the predicted value of y which is $\\overline{y}$, and y is the actual value,pred_y is the predicted value of y which is $\\overline{y_i}=ax_i + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4053045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y,pred_y):\n",
    "    #here X and y are the inputs and the pred_y is the predicted value for y\n",
    "    n=len(X) #total number of elements in X\n",
    "    grad_a = -(2/n) * sum(X * (y - pred_y)) #derivative of the loss function with respect to a\n",
    "                                            #  y-pred_y will be an array and then multiplied component wise with array X \n",
    "                                            #and then summed similarly below\n",
    "    grad_b = -(2/n) * sum(y - pred_y) #derivative of the loss function with respect to b\n",
    "    return grad_a, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88dad4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X=2.5*np.random.randn(10000)+1.5\n",
    "res=1.5*np.random.randn(10000)\n",
    "y=2+0.3*X+res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccd18bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.c\n",
    "def grad_desc(X,y):\n",
    "    n=float(len(X))\n",
    "    a=0 #initial value of parameter a in y=ax+b is kept zero\n",
    "    b=0 #initial value of parameter b in y=ax+b is kept zero\n",
    "    l=0.001 \n",
    "    count=10500\n",
    "    '''\n",
    "        for this learning rate l the number of iterations count is kept 21500, \n",
    "        after running it we will see that at minima, \n",
    "        the cost value will not reduce much,giving the expected values of parameters a and b\n",
    "    '''    \n",
    "    for i in range(count):\n",
    "        y_pred = a*X + b # this is the predicted value of y\n",
    "#         cost = (1/n) * sum([val**2 for val in (y-y_pred)])\n",
    "        '''\n",
    "        here cost is the mean square error,\n",
    "        which is just the sum of squares of the (y-y_pred) values\n",
    "        '''\n",
    "        grad_a,grad_b = gradient(X,y,y_pred) # here we will get the partial derivatives w.r.t to a and b which is the direction\n",
    "        a-=grad_a*l \n",
    "        b-=grad_b*l\n",
    "        '''\n",
    "        here we are updating the values a and b ,\n",
    "        this update changes the value of a and b on the curve with respect to the slope from the previous step,\n",
    "        and when the minima is attained they will reduce much and get stuck on a particular value\n",
    "        '''\n",
    "#         print('a {} , b {}, cost {}, iteration {}'.format(a,b,cost,i)) # this is to fine tune the parameters l and count\n",
    "    return a,b # this will return the a and b values which give the least error and fit the data well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25350fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.29531861789670527, 2.023287961712447)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grad_desc(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b46b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.d\n",
    "#this function takes 3 parameters inputs X and y and batch size\n",
    "def minibatchgradientdescent(X,y,batch):\n",
    "    a=0 #initial value of parameter a\n",
    "    b=0 #initial value of parameter b\n",
    "    l=0.001 # this is learning rate which controls how much the parameters a and b change\n",
    "    count=10500 # number of times our loop will run\n",
    "    for i in range(count):\n",
    "        idx=np.random.randint(0,len(X),batch) # here we are sampling a mini-batch of size batch\n",
    "        x_batch = X[idx] # idx is the random indexes of size batch with which we sample x_batch and y_batch\n",
    "        y_batch = y[idx] # x_batch and y_batch will be an array of random batch size\n",
    "        y_pred = a*x_batch + b # this is the predicted value of y using the y_batch\n",
    "        error = (1/batch) * sum([val**2 for val in (y_batch-y_pred)])\n",
    "        grad_a,grad_b= gradient(x_batch,y_batch,y_pred) # here we are calculating the gradients \n",
    "                                                        # with respect to the sampled x_batch and y_batch\n",
    "        a-=grad_a*l # here a and b gets updates according to the slope found from the previous step \n",
    "        b-=grad_b*l # when minima is attained they stop changing\n",
    "    return a,b,error # this returns the expected values of a and b which helps in fitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0a97cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 429 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3052155335220458, 2.014733929387673, 2.2348251092648814)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "minibatchgradientdescent(X,y,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e30bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_size = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cdd65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 e part 1\n",
    "#sgd will take three parameters the input X and y with size(how many datapoints to choose)\n",
    "def SGD(X,y,size):\n",
    "    a=0\n",
    "    b=0\n",
    "    l=0.001\n",
    "    count=10500\n",
    "    for i in range(count):\n",
    "        idx = np.random.randint(arr_size)#idx will have only \n",
    "                                        # one index chosen randomly between \n",
    "                                        # 0 and len(X)-1 so x_sample and \n",
    "                                        # y_sample below would only have one data point\n",
    "        x_sample = X[idx] \n",
    "        y_sample = y[idx]\n",
    "        y_pred = a*x_sample + b # predicted value of y\n",
    "        grad_a = (-2/size) * (x_sample*(y_sample - y_pred))\n",
    "        grad_b = (-2/size) * (y_sample - y_pred)\n",
    "        a = a-grad_a*l #updation of model parameters according to the slope found in the above step\n",
    "        b = b-grad_b*l\n",
    "    return a,b # will return a,b which helps fit the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58b90135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.37082468353723386, 1.9934367480646797)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SGD(X,y,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6522ff",
   "metadata": {},
   "source": [
    "we can see that for the same initial values of parameter a,b,learning rate and iterations count stochastic Gradient descent performs better in terms of time than vanilla gradient descent because SGD picks up only one data point and finds the gradient with it taking lesser time than GD who takes every datapoint, the execution time of GD and SGD varies a lot,if we lower the iteration count time would be reduced but the model parameters a and b will be calulcated with less accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90495f7",
   "metadata": {},
   "source": [
    "1e part2<br>\n",
    "the optimal minibatch size should be the size which gives us the  values which are close to the expected values of parameters a and b which (a,b) ~ (0.3,2) which we can find out by getting all the a , b values for all possible batch sizes and checking the batch size which gives the lowest error we will start from 2 and the maximum we can go till len(X) , excluding it because if we take the whole data set then it is no longer mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14c4c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (5.0, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0360c8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "error_values = []\n",
    "batch_values = []\n",
    "size = len(X)\n",
    "batch = 2\n",
    "while(batch <= size):\n",
    "    a,b,error = minibatchgradientdescent(X,y,batch)\n",
    "    error_values.append(error)\n",
    "    batch_values.append(batch)\n",
    "    batch*=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10951315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal batch size is : 8 having error of : 1.3776542226120672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'error')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "minpos = error_values.index(min(error_values))\n",
    "print('optimal batch size is : {} having error of : {}'.format(batch_values[minpos],error_values[minpos])) \n",
    "plt.plot(batch_values,error_values)\n",
    "plt.xlabel('batch size')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e222da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
