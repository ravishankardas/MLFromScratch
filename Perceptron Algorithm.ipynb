{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7339c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries that will be used \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d65b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data available is as a text file so i've assigned the column headers for the dataframe \n",
    "data = pd.read_csv('data_banknote_authentication.txt',header=None)\n",
    "data.columns = [\"variance\", \"skewness\", \"curtosis\", \"entropy \",\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d92440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e803806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the class labels -1 where we have 0 in the dataset\n",
    "df.loc[df['class'] == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4593610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-1.386115</td>\n",
       "      <td>-0.997144</td>\n",
       "      <td>0.399741</td>\n",
       "      <td>-1.109658</td>\n",
       "      <td>-0.110787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.326021</td>\n",
       "      <td>3.602272</td>\n",
       "      <td>3.840276</td>\n",
       "      <td>1.385713</td>\n",
       "      <td>0.994207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-7.042100</td>\n",
       "      <td>-13.773100</td>\n",
       "      <td>-5.286100</td>\n",
       "      <td>-7.588700</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.563600</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.247285</td>\n",
       "      <td>-0.084047</td>\n",
       "      <td>-0.993970</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.391700</td>\n",
       "      <td>9.601400</td>\n",
       "      <td>17.927400</td>\n",
       "      <td>2.135300</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          variance     skewness     curtosis     entropy         class\n",
       "count  1372.000000  1372.000000  1372.000000  1372.000000  1372.000000\n",
       "mean     -1.386115    -0.997144     0.399741    -1.109658    -0.110787\n",
       "std       1.326021     3.602272     3.840276     1.385713     0.994207\n",
       "min      -7.042100   -13.773100    -5.286100    -7.588700    -1.000000\n",
       "25%      -1.563600    -1.000000    -1.000000    -1.000000    -1.000000\n",
       "50%      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000\n",
       "75%      -1.000000    -0.247285    -0.084047    -0.993970     1.000000\n",
       "max       2.391700     9.601400    17.927400     2.135300     1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here the we can see a significant difference in the max values\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ca400",
   "metadata": {},
   "source": [
    "1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e447a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes two arguments m = array of mistakes k = artificial limit and return if the algorithm has converged\n",
    "def converge(m,k):\n",
    "    for i in range(len(m)-k+1):\n",
    "        temp = [] # temorary array to check the sum\n",
    "        for j in range(i,i+k):#we take every elements in the range(i,i+k) from mistakes array and store it in the temp and if the sum of element of array temp is zero then all the elements were zero\n",
    "            temp.append(m[j])# meaning there were no updates for previos k iterations at the current iteration of the algorithm                            \n",
    "        if(sum(temp)==0):#if sum is 0 meaning the algorithm has converged and we return true and the algorithm below stops loop and returns the current w\n",
    "            return True\n",
    "    return False # if not converged then we continue the algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2642abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the Stochastic sub gradient descent which takes 4 arguments x: dataset,y=label,epoch = number of times the \n",
    "#for loop runs and return the learned weights\n",
    "def ssgd(X,Y,epoch,k):\n",
    "    mistakes = []\n",
    "    W = np.zeros(X.shape[1])# here we make an array of zeros of size equal to the number of columns of the passed dataset\n",
    "    for j in range(epoch):\n",
    "        index = np.random.randint(0,X.shape[0])# here we find a random index between 0 and number of rows of the given dataset\n",
    "        x = X.iloc[index].to_numpy()#after finding the index we make an array x with the row of the dataset at that index\n",
    "        y = Y[index]# we take the class label from the same index of passed label     \n",
    "        value = np.dot(W,x)*y\n",
    "        if(value <= 0): #here we check if we made a mistake , here i've used <= rather < because the if the value is 0 then the weights will never change\n",
    "            mistakes.append(1) # we append a 1 if there's no mistake in that iteration          \n",
    "            W = W + y*x\n",
    "        else:\n",
    "            mistakes.append(0)# we append a 0 if there's no mistake in that iteration\n",
    "        if(converge(mistakes,k)==True):# if converged we return that W\n",
    "            return W\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b613b5",
   "metadata": {},
   "source": [
    "1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09107ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here only the features need to be normalised the class label is skipped, z-score normalisation is used where the \n",
    "# mean of the column is subtracted from each datapoint and divided by the standard deviation of the column \n",
    "new_col = [\"variance\", \"skewness\", \"curtosis\", \"entropy \"]\n",
    "for col in new_col:\n",
    "    df[col] = (df[col]-df[col].mean())/df[col].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea833d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes two arguments w is the calculated weight and x is the row of the dataset for which the weight w is \n",
    "#calculated and returns the lable for the given weight and and the row \n",
    "def predict(w,x):\n",
    "    if np.dot(w,x)<0:#here if the dot product is less 0 we predict -1 label else we predict 1 label\n",
    "        return -1\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d9bc433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for test/train split\n",
    "new_df = df.sample(frac=1)#first we shuffle the whole dataset\n",
    "train_size = int(0.8 * len(df))# this is size upon which we will split the entire dataset\n",
    "train_set = new_df[:train_size].copy()# train set which will include the top 80% data\n",
    "test_set = new_df[train_size:].copy()# test set which will include the  last 20% data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c2587b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = [] # this list will score the error values when we will test with the k-th held out set\n",
    "global_weights = [] # this list will store the weights which was used for predicting the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7681451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function stores the error in each iteration of the loop in the score list above and takes two arguments\n",
    "# test is the unseen test set on which the error will be calculated and w is the learned weight of that fold \n",
    "def evaluation(test,weights):\n",
    "    error = 0 # to get the total error for the current test set\n",
    "    act = []# stores the actual labels\n",
    "    pred = [] # stores the predicted labels\n",
    "    for i,j in test.iterrows():# we iterate over every row\n",
    "        x = j.tolist()# we make a list of the current row\n",
    "        actual_y = int(x[-1]) # we get the actual label from the list above\n",
    "        x = x[:-1] # now that we have taken the actual class label we will discard the last element to match the requirements\n",
    "        pred_y = predict(weights,x) # here we will get the prediction for the learned weight and the current row\n",
    "        act.append(actual_y)#storing the label of the current row\n",
    "        pred.append(pred_y)#storing the predicted label for the the current row\n",
    "    for k in range(len(pred)):\n",
    "        error+=max(0,-act[k]*pred[k]) #in each iteration we calcuate the perceptron loss and add it to the local variable error\n",
    "    score.append(error)#append the overall error in the score list \n",
    "    global_weights.append(weights) # stores the weights for which prediction was made"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6209545",
   "metadata": {},
   "source": [
    "1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6eaea760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation takes two arguments as df = dataset for which cv is to performed and k is the number of folds\n",
    "#this function will update the scores and the global weights above\n",
    "def CV(df,k):\n",
    "    folds = np.array_split(df.sample(frac=1), k)# we use the numpy.array_split() to divide the passed dataset df into k folds\n",
    "    for i in folds:#iterate over every fold\n",
    "        test = i # current fold(current small dataset) will be used for testing\n",
    "        train = pd.DataFrame()#intilise a blank dataframe to later store the (k-1) folds for training\n",
    "        for j in folds:\n",
    "            if(j.equals(test)==False):#here we append the current dataset into train above if they dont match\n",
    "                train = train.append(j,ignore_index=True)\n",
    "        new_train = train.copy(deep=True)\n",
    "        y = new_train['class'].to_list()#we take the last column class as a list in y\n",
    "        new_train.drop(new_train.columns[len(new_train.columns)-1], axis=1, inplace=True)# drop the last column to \n",
    "                                                                                        # meet the requirements\n",
    "        new_train.reset_index(inplace=True)#reset index because in the ssgd function index plays a significant role\n",
    "        new_train.pop('index')#a index columns is added to the front of the dataset which is removed\n",
    "        weights = ssgd(new_train,y,100,6)#here we learn the weight for the current train set\n",
    "        evaluation(test,weights) #this function call will update the score anf global_wights list above for \n",
    "                                #the held out test set       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "158c4aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_26076\\4253517960.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train = train.append(j,ignore_index=True)\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_26076\\4253517960.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train = train.append(j,ignore_index=True)\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_26076\\4253517960.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train = train.append(j,ignore_index=True)\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_26076\\4253517960.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train = train.append(j,ignore_index=True)\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_26076\\4253517960.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train = train.append(j,ignore_index=True)\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_26076\\4253517960.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train = train.append(j,ignore_index=True)\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_26076\\4253517960.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train = train.append(j,ignore_index=True)\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_26076\\4253517960.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train = train.append(j,ignore_index=True)\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_26076\\4253517960.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train = train.append(j,ignore_index=True)\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_26076\\4253517960.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train = train.append(j,ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# CV funstion is called on the train_set(80% of the dataset) with k=10(it is recommedend for k=10 for larger datasets) \n",
    "#so we can get the score of the k folds , for every fold that we make we train on the other (k-1) folds to get the \n",
    "#weights after which we test on the held out fold for the current iteration to get the error and update the score list\n",
    "CV(train_set,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52c1d934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error on the train set is 19.6\n"
     ]
    }
   ],
   "source": [
    "# mean value of the error\n",
    "print('Mean error on the train set is {}'.format(sum(score)/len(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f5355ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index of the minimum value element of score list\n",
    "min_idx = score.index(min(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a4e3135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the size of score and global_weights lists are same we can get for which weight we get the least error\n",
    "optimal_weights = global_weights[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be23b98",
   "metadata": {},
   "source": [
    "1.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5518c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function finds the f1 score on the held out set test_set(20% of the wohole dataset) and takes two arguments\n",
    "# test which is the unseen data and the optimal weight of our best model(least error)\n",
    "def calculate(test,weights):\n",
    "    tp = 0 # for keeping track of true postive predictions\n",
    "    fp = 0 # for keeping track of false postive predictions\n",
    "    fn = 0 # for keeping track of false negative predictions\n",
    "    for i,j in test.iterrows(): # we iterate through every row of the test set\n",
    "        x = j.tolist() # take a row of the test_set\n",
    "        actual_y = int(x[-1]) #take the label\n",
    "        x = x[:-1] #remove the last element(class label)\n",
    "        pred_y = predict(weights,x) # get the predicted label for the optimal weight and current row\n",
    "        if actual_y == 1:\n",
    "            if pred_y == 1:\n",
    "                tp+=1  # update the tp value if the actual lable belongs to the positive class and the prediction \n",
    "                        #is also positive\n",
    "            else:\n",
    "                fp+=1 #update the fp value if the actual lable belongs to the positive class but the prediction is negative\n",
    "        elif actual_y == -1 & pred_y == 1:\n",
    "            fn+=1 # update the fn value if the actual lable belongs to the negative class but the prediction is positive\n",
    "    p = tp/(tp+fp) # precision p is the number of correctly classified positive examples divided by the total number \n",
    "                   #of examples that are classified as positive\n",
    "    r = tp/(tp+fn) # recall r is the number of correctly classified positive examples divided by the total number of \n",
    "                    #actual positive examples in the test set\n",
    "    return(2*p*r/(p+r)) # harmonic mean of precision and recall will give us the accuracy of the model on the \n",
    "                        #dataset(held out test_set)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c9aea29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1-score is 0.7729468599033816\n"
     ]
    }
   ],
   "source": [
    "#here we will get the f1-score , higher the f1-score better the classifier\n",
    "print('The F1-score is {}'.format(calculate(test_set,optimal_weights)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7af7df4901773a0e355da496bf365ae011b1b331a57bbc9908dae1ee21823d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
